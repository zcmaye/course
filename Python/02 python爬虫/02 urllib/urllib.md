# Urllib

`urllib`æ˜¯ Python æ ‡å‡†åº“ä¸­ç”¨äºå¤„ç† URL çš„æ¨¡å—ï¼Œå®ƒæä¾›äº†ä¸€ç³»åˆ—ç”¨äºå¤„ç† URL çš„å·¥å…·ï¼Œèƒ½å¤Ÿå®ç°ä»ç®€å•çš„ URL è¯»å–åˆ°å¤æ‚çš„ HTTP è¯·æ±‚ç­‰æ“ä½œã€‚

urllibåº“æä¾›äº†å››å¤§æ¨¡å—ï¼š

- **`urllib.request`**ï¼šç”¨äºæ‰“å¼€å’Œè¯»å– URLï¼Œæä¾›äº†é«˜çº§çš„æ¥å£æ¥å¤„ç†ä¸åŒç±»å‹çš„è¯·æ±‚ï¼Œåƒ HTTPã€FTP ç­‰åè®®çš„è¯·æ±‚ã€‚
- **`urllib.error`**ï¼šåŒ…å«äº†`urllib.request`æŠ›å‡ºçš„å¼‚å¸¸ï¼Œå¯ç”¨äºæ•è·å’Œå¤„ç†è¯·æ±‚è¿‡ç¨‹ä¸­å‡ºç°çš„é”™è¯¯ã€‚
- **`urllib.parse`**ï¼šç”¨äºè§£æå’Œå¤„ç† URLï¼Œä¾‹å¦‚æ‹†åˆ† URLã€åˆå¹¶ URLã€ç¼–ç å’Œè§£ç  URL å‚æ•°ç­‰ã€‚
- **`urllib.robotparser`**ï¼šç”¨äºè§£æç½‘ç«™çš„`robots.txt`æ–‡ä»¶ï¼Œä»¥æ­¤åˆ¤æ–­çˆ¬è™«æ˜¯å¦è¢«å…è®¸è®¿é—®è¯¥ç½‘ç«™çš„ç‰¹å®šé¡µé¢ï¼Œä½¿ç”¨é¢‘ç‡è¾ƒå°‘ã€‚

## urllib.parse

### è§£æURL(`urlsplit`)

`urlparse`å‡½æ•°å¯ä»¥å°†ä¸€ä¸ª URL æ‹†åˆ†ä¸ºå¤šä¸ªç»„ä»¶ï¼Œå¦‚åè®®ã€åŸŸåã€è·¯å¾„ã€æŸ¥è¯¢å‚æ•°ç­‰ã€‚

```python
url = 'https://www.macrodef.com/path/to/resource?user_id=2&username=ç©è›‡#fragment'

parse_result = urllib.parse.urlsplit(url)

print(f"åè®®: {parse_result.scheme}")
print(f"åŸŸå: {parse_result.netloc}")
print(f"è·¯å¾„: {parse_result.path}")
print(f"æŸ¥è¯¢å‚æ•°: {parse_result.query}")
print(f"ç‰‡æ®µ: {parse_result.fragment}")
```

åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼Œ`urlparse`å‡½æ•°å°† URL æ‹†åˆ†ä¸ºå„ä¸ªç»„ä»¶ï¼Œå­˜å‚¨åœ¨`parsed_url`å¯¹è±¡ä¸­ï¼Œé€šè¿‡è®¿é—®è¯¥å¯¹è±¡çš„å±æ€§å¯ä»¥è·å–å„ä¸ªç»„ä»¶çš„å€¼ã€‚

> è¿˜æœ‰ä¸€ä¸ªurlparserå‡½æ•°åŠŸèƒ½ä¸urlsplitç±»ä¼¼ï¼Œåªä¸è¿‡ä¼šå¤šå¤„ç†ä¸€ä¸ªparamï¼Œä½†æ˜¯ç°ä»£URLä¸€å®šä¸ä¼šç”¨åˆ°ï¼Œæ‰€ä»¥ä¸å¿…ä½¿ç”¨urlparserï¼

### åˆå¹¶ URL ç»„ä»¶ (`urlunsplit`)

`urlunparse`å‡½æ•°ä¸`urlparse`ç›¸åï¼Œå®ƒå¯ä»¥å°†å¤šä¸ª URL ç»„ä»¶åˆå¹¶æˆä¸€ä¸ªå®Œæ•´çš„ URLã€‚

```python
components = ['https','www.hdy.com','/path/to/resource','user_id=2&username=ç©è›‡','fragment']

url = urllib.parse.urlunsplit(components)

print(f'url {url}')
```

æ­¤ä»£ç ä¸­ï¼Œå°†åŒ…å«åè®®ã€åŸŸåã€è·¯å¾„ã€æŸ¥è¯¢å‚æ•°å’Œç‰‡æ®µçš„å…ƒç»„ä¼ é€’ç»™`urlunparse`å‡½æ•°ï¼Œå®ƒä¼šå°†è¿™äº›ç»„ä»¶åˆå¹¶æˆä¸€ä¸ªå®Œæ•´çš„ URLã€‚

> è¿˜æœ‰ä¸€ä¸ªurlunparserå‡½æ•°åŠŸèƒ½ä¸urlunsplitç±»ä¼¼ï¼Œåªä¸è¿‡ä¼šå¤šå¤„ç†ä¸€ä¸ªparamï¼Œä½†æ˜¯ç°ä»£URLä¸€å®šä¸ä¼šç”¨åˆ°ï¼Œæ‰€ä»¥ä¸å¿…ä½¿ç”¨urlunparserï¼

### ç¼–è§£ç URLå‚æ•°

åœ¨æµè§ˆå™¨ä¸­è®¿é—®URLæ—¶ï¼Œä¼šå‘ç°æµè§ˆå™¨æŠŠURLä¸­çš„ä¸­æ–‡å’Œä¸€äº›å­—ç¬¦è½¬æˆäº†ç±»ä¼¼äº`%E4%BD%A0`çš„ç¼–ç ï¼Œä¸ºä»€ä¹ˆå‘¢ï¼Ÿé¦–å…ˆä¸æ˜¯ä¹±ç¼–ç çš„ï¼Œæœ‰ä¸€ä¸ªç¼–ç å«åšURL ç¼–ç ï¼ˆPercent-Encodingï¼Œä¹Ÿå«ç™¾åˆ†å·ç¼–ç ï¼‰ï¼Œæ˜¯ä¸ºäº† **è®© URL å¯ä»¥å®‰å…¨ã€æ­£ç¡®åœ°ä¼ è¾“å’Œè§£æ**ï¼Œä¸»è¦è§£å†³ä»¥ä¸‹å‡ ä¸ªé—®é¢˜ï¼š

##### 1. ä¸ºä»€ä¹ˆéœ€è¦ URL ç¼–ç ï¼Ÿ

URL åªèƒ½ä½¿ç”¨ **æœ‰é™å­—ç¬¦é›†**ï¼ˆASCII å­—ç¬¦ï¼‰ï¼Œä½†å®é™…ä½¿ç”¨ä¸­å¯èƒ½åŒ…å«ï¼š

- **é ASCII å­—ç¬¦**ï¼ˆå¦‚ä¸­æ–‡ã€æ—¥æ–‡ã€Emojiï¼‰
- **ç‰¹æ®Šå­—ç¬¦**ï¼ˆå¦‚ `ç©ºæ ¼`ã€`?`ã€`#`ã€`&`ã€`=` ç­‰ï¼Œå®ƒä»¬åœ¨ URL ä¸­æœ‰ç‰¹æ®Šå«ä¹‰ï¼‰
- **ä¿ç•™å­—ç¬¦**ï¼ˆå¦‚ `/`, `:`, `@` ç­‰ï¼Œç”¨äº URL ç»“æ„ï¼‰

å¦‚æœä¸ç¼–ç ï¼ŒURL å¯èƒ½ä¼š **è§£æé”™è¯¯** æˆ– **ä¼ è¾“å¤±è´¥**ã€‚

##### 2. URL ç¼–ç çš„è§„åˆ™

- **ä¿ç•™å­—ç¬¦**ï¼ˆå¦‚ `?`, `=`, `/`, `#`ï¼‰åœ¨ç‰¹å®šä½ç½®æœ‰ç‰¹æ®Šå«ä¹‰ï¼Œä¸èƒ½ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™ä¼šè¢«è§£ææˆ URL çš„ç»“æ„éƒ¨åˆ†ã€‚
- **éä¿ç•™å­—ç¬¦**ï¼ˆå¦‚å­—æ¯ `A-Za-z`ã€æ•°å­— `0-9` å’Œ `-_.~`ï¼‰**ä¸éœ€è¦ç¼–ç **ã€‚
- **å…¶ä»–å­—ç¬¦**ï¼ˆå¦‚ç©ºæ ¼ã€ä¸­æ–‡ã€`&`ã€`%`ï¼‰å¿…é¡»è½¬æ¢æˆ `%` + **åå…­è¿›åˆ¶ ASCII ç **ï¼š
  - ç©ºæ ¼ â†’ `%20`ï¼ˆæˆ–è€… `+`ï¼Œä½†åœ¨æŸ¥è¯¢å‚æ•°é‡Œ `+` ä¼šè¢«è§£ç æˆç©ºæ ¼ï¼‰
  - ä¸­æ–‡ "ä½ " â†’ `%E4%BD%A0`ï¼ˆUTF-8 ç¼–ç çš„åå…­è¿›åˆ¶è¡¨ç¤ºï¼‰
  - `&` â†’ `%26`ï¼ˆå¦åˆ™ä¼šè¢«è§£æä¸ºå‚æ•°åˆ†éš”ç¬¦ï¼‰

åœ¨è·¯å¾„(path)ã€æŸ¥è¯¢å‚æ•°(query)ã€é”šç‚¹(fragment)ä¸­ï¼Œå¿…é¡»è¿›è¡Œç¼–ç ï¼

```css
#path
http://example.com/æ–‡ä»¶.pdf âŒï¼ˆä¸èƒ½ç›´æ¥ä½¿ç”¨ä¸­æ–‡ï¼‰
http://example.com/%E6%96%87%E4%BB%B6.pdf âœ…ï¼ˆç¼–ç åï¼‰
#query
http://example.com/search?q=python&price=100 âŒï¼ˆ`&` ä¼šè¢«è§£ææˆåˆ†éš”ç¬¦ï¼‰
http://example.com/search?q=python%26price%3D100 âœ…ï¼ˆ`&` â†’ `%26`ï¼Œ`=` â†’ `%3D`ï¼‰
#fragment
http://example.com/page#ç« èŠ‚1 âŒ
http://example.com/page#%E7%AB%A0%E8%8A%821 âœ…
```

##### 3. ç¼–ç (quote/quote_plus/urlencode)

###### quote

å‡½æ•°åŸå‹ï¼š

```python
def quote(string, safe='/', encoding=None, errors=None) -> str
```

- **ç”¨é€”**ï¼šå¯¹ **å•ä¸ªå­—ç¬¦ä¸²** è¿›è¡Œ URL ç¼–ç ï¼ˆPercent-Encodingï¼‰ã€‚

- **ç‰¹ç‚¹**ï¼š

  - **é»˜è®¤ä¸ç¼–ç æ–œæ  `/`**ï¼ˆé€‚ç”¨äºè·¯å¾„éƒ¨åˆ†ï¼‰ã€‚
  - **ç©ºæ ¼** ``ç¼–ç ä¸º`**%20**ï¼ˆä¸æ˜¯ `+`ï¼‰ã€‚
  - é€‚ç”¨äº **è·¯å¾„ï¼ˆPathï¼‰** æˆ– **Fragmentï¼ˆé”šç‚¹ï¼‰** çš„ç¼–ç ã€‚

- **ç¤ºä¾‹:**

  ```python
  from urllib.parse import quote
  
  # ç¼–ç è·¯å¾„ï¼ˆä¿ç•™ /ï¼‰
  print(quote("æ–‡æ¡£/æµ‹è¯•.pdf"))  
  # è¾“å‡º: %E6%96%87%E6%A1%A3/%E6%B5%8B%E8%AF%95.pdf
  
  # ç¼–ç ç‰¹æ®Šå­—ç¬¦
  print(quote("a b=c&d"))  
  # è¾“å‡º: a%20b%3Dc%26dï¼ˆç©ºæ ¼â†’%20ï¼Œ=â†’%3Dï¼Œ&â†’%26ï¼‰
  ```

###### quote_plus

å‡½æ•°åŸå‹:

```python
def quote_plus(string, safe='', encoding=None, errors=None) -> str
```

- **ç”¨é€”**ï¼šå¯¹ **æŸ¥è¯¢å‚æ•°ï¼ˆQuery Stringï¼‰** è¿›è¡Œç¼–ç ã€‚

- **ç‰¹ç‚¹**ï¼š

  - **ç©ºæ ¼** `ç¼–ç ä¸º `**+**ï¼ˆç¬¦åˆ application/x-www-form-urlencoded` æ ‡å‡†ï¼‰ã€‚
  - **`/` ä¹Ÿä¼šè¢«ç¼–ç ä¸º `%2F`**ï¼ˆé™¤éæ‰‹åŠ¨è®¾ç½® `safe='/'`ï¼‰ã€‚
  - é€‚ç”¨äº **URL æŸ¥è¯¢å‚æ•°ï¼ˆQuery Stringï¼‰** æˆ– **POST è¡¨å•æ•°æ®**ã€‚

- **ç¤ºä¾‹ï¼š**

  ```python
  from urllib.parse import quote_plus
  
  # ç¼–ç æŸ¥è¯¢å‚æ•°ï¼ˆç©ºæ ¼â†’+ï¼‰
  print(quote_plus("a b=c&d"))  
  # è¾“å‡º: a+b%3Dc%26dï¼ˆç©ºæ ¼â†’+ï¼Œ=â†’%3Dï¼Œ&â†’%26ï¼‰
  
  # å¯¹æ¯” quote()
  print(quote("a b=c&d"))  
  # è¾“å‡º: a%20b%3Dc%26d
  ```

###### urlencode

å‡½æ•°åŸå‹ï¼š

```python
def urlencode(query, doseq=False, safe='', encoding=None, errors=None,quote_via=quote_plus) -> str
```

- **ç”¨é€”**ï¼šå°† **å­—å…¸æˆ–é”®å€¼å¯¹åˆ—è¡¨** ç¼–ç ä¸º URL æŸ¥è¯¢å­—ç¬¦ä¸²ï¼ˆ`?key1=val1&key2=val2`ï¼‰ã€‚

- **ç‰¹ç‚¹**ï¼š

  - è‡ªåŠ¨å¤„ç† **é”®å’Œå€¼** çš„ç¼–ç ï¼ˆç›¸å½“äºè°ƒç”¨äº† `quote_plus`ï¼‰ã€‚
  - æ”¯æŒåµŒå¥—æ•°æ®ï¼ˆé€šè¿‡ `doseq=True` å¤„ç†åˆ—è¡¨å€¼ï¼‰ã€‚

- ç¤ºä¾‹ï¼š

  ```python
  from urllib.parse import urlencode
  
  # å­—å…¸ â†’ æŸ¥è¯¢å­—ç¬¦ä¸²
  params = {"q": "python æ•™ç¨‹", "page": 1}
  print(urlencode(params))  
  # è¾“å‡º: q=python+%E6%95%99%E7%A8%8B&page=1ï¼ˆç©ºæ ¼â†’+ï¼Œä¸­æ–‡â†’%E6%95%99%E7%A8%8Bï¼‰
  
  # å…ƒç»„åˆ—è¡¨ â†’ æŸ¥è¯¢å­—ç¬¦ä¸²
  params_list = [("q", "python"), ("page", 1), ("sort", "newest")]
  print(urlencode(params_list))  
  # è¾“å‡º: q=python&page=1&sort=newest
  
  # å¤„ç†å¤šå€¼ï¼ˆdoseq=Trueï¼‰
  multi_params = {"q": ["python", "java"], "page": 1}
  print(urlencode(multi_params, doseq=True))  
  # è¾“å‡º: q=python&q=java&page=1
  ```

###### æ€»ç»“

- **`quote()`**ï¼šè·¯å¾„ç¼–ç ï¼ˆ`/` ä¸ç¼–ç ï¼Œç©ºæ ¼â†’`%20`ï¼‰ã€‚
- **`quote_plus()`**ï¼šæŸ¥è¯¢å‚æ•°å€¼ç¼–ç ï¼ˆç©ºæ ¼â†’`+`ï¼Œ`/` ç¼–ç ï¼‰ã€‚
- **`urlencode()`**ï¼šç›´æ¥ç”Ÿæˆå®Œæ•´æŸ¥è¯¢å­—ç¬¦ä¸²ï¼ˆè‡ªåŠ¨è°ƒç”¨ `quote_plus`ï¼‰ã€‚

æ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚çš„æ–¹æ³•ï¼Œç¡®ä¿ URL æ­£ç¡®ã€å®‰å…¨åœ°ä¼ è¾“ï¼ ğŸš€

##### 4 è§£ç (unquote/unquote_plus/parse_qs[l])

###### unquote

å‡½æ•°åŸå‹ï¼š

```python
def unquote(string: str | bytes, encoding: str = "utf-8", errors: str = "replace") -> str
```

- **åŠŸèƒ½**ï¼šå°† URL ç¼–ç çš„å­—ç¬¦ä¸²ï¼ˆ`%xx` å½¢å¼ï¼‰è§£ç å›åŸå§‹å­—ç¬¦ä¸²ã€‚

- **ç‰¹ç‚¹**ï¼š

  - **`%20` è§£ç ä¸ºç©ºæ ¼**ï¼ˆ` ç©ºæ ¼ `ï¼‰ã€‚
  - **ä¸å¤„ç† `+`**ï¼ˆ`+` ä¸ä¼šè¢«è½¬æ¢ä¸ºç©ºæ ¼ï¼‰ã€‚
  - é€‚ç”¨äº **è·¯å¾„ï¼ˆPathï¼‰ã€Fragmentï¼ˆé”šç‚¹ï¼‰** çš„è§£ç ã€‚

- **ç¤ºä¾‹ï¼š**

  ```python
  from urllib.parse import unquote
  
  encoded_str = "%E4%BD%A0%E5%A5%BD%20World%21"
  decoded_str = unquote(encoded_str)
  print(decoded_str)  # è¾“å‡º: "ä½ å¥½ World!"
  ```

  

###### unquote_plus

å‡½æ•°åŸå‹ï¼š

```python
def unquote_plus(string: str, encoding: str = "utf-8", errors: str = "replace") -> str
```

- **åŠŸèƒ½**ï¼šè§£ç  URL ç¼–ç çš„å­—ç¬¦ä¸²ï¼Œ**é¢å¤–å°† `+` è½¬æ¢ä¸ºç©ºæ ¼**ã€‚

- **ç‰¹ç‚¹**ï¼š

  - **`%20` å’Œ `+` éƒ½ä¼šè§£ç ä¸ºç©ºæ ¼**ã€‚
  - é€‚ç”¨äº **æŸ¥è¯¢å‚æ•°ï¼ˆQuery Stringï¼‰** çš„è§£ç ï¼ˆå› ä¸ºæŸ¥è¯¢å‚æ•°ä¸­çš„ç©ºæ ¼é€šå¸¸ç¼–ç ä¸º `+`ï¼‰ã€‚

- **ç¤ºä¾‹ï¼š**

  ```python
  from urllib.parse import unquote_plus
  
  encoded_str = "q=python+%26+java&page=1"
  decoded_str = unquote_plus(encoded_str)
  print(decoded_str)  # è¾“å‡º: "q=python & java&page=1"
  ```

  

###### parse_qs

å‡½æ•°åŸå‹ï¼š

```python
def parse_qs(qs, keep_blank_values=False, strict_parsing=False,
             encoding='utf-8', errors='replace', max_num_fields=None, separator='&')->dict
```

- **åŠŸèƒ½**ï¼šå°† **æŸ¥è¯¢å­—ç¬¦ä¸²ï¼ˆ`?key1=val1&key2=val2`ï¼‰è§£ç ä¸ºå­—å…¸**ã€‚

- **ç‰¹ç‚¹**ï¼š

  - è¿”å› `Dict[str, List[str]]`ï¼ˆå€¼æ€»æ˜¯åˆ—è¡¨ï¼Œæ”¯æŒå¤šå€¼å‚æ•°ï¼‰ã€‚
  - è‡ªåŠ¨è°ƒç”¨ `unquote_plus()` è§£ç é”®å’Œå€¼ï¼ˆ`+` â†’ ç©ºæ ¼ï¼Œ`%xx` â†’ å­—ç¬¦ï¼‰ã€‚

- **ç¤ºä¾‹ï¼š**

  ```python
  from urllib.parse import parse_qs
  
  query_string = "q=python+%26+java&page=1&lang=en&lang=zh"
  result = parse_qs(query_string)
  print(result)
  ```

  è¾“å‡ºï¼š

  ```python
  {
      'q': ['python & java'],  # + å’Œ %26 è¢«è§£ç 
      'page': ['1'],
      'lang': ['en', 'zh']    # å¤šå€¼åˆå¹¶ä¸ºåˆ—è¡¨
  }
  ```

  

###### parse_qsl

å‡½æ•°åŸå‹ï¼š

```python
def parse_qsl(qs, keep_blank_values=False, strict_parsing=False,
              encoding='utf-8', errors='replace', max_num_fields=None, separator='&')->list
```

- **åŠŸèƒ½**ï¼šå°† **æŸ¥è¯¢å­—ç¬¦ä¸²è§£ç ä¸ºå…ƒç»„åˆ—è¡¨**ï¼Œä¿ç•™é‡å¤é”®ã€‚

- **ç‰¹ç‚¹**ï¼š

  - è¿”å› `List[Tuple[str, str]]`ï¼ˆé€‚åˆéœ€è¦ä¿æŒé¡ºåºæˆ–å¤„ç†é‡å¤é”®çš„åœºæ™¯ï¼‰ã€‚
  - åŒæ ·è‡ªåŠ¨è°ƒç”¨ `unquote_plus()` è§£ç ã€‚

- **ç¤ºä¾‹ï¼š**

  ```python
  from urllib.parse import parse_qsl
  
  query_string = "q=python+%26+java&page=1&lang=en&lang=zh"
  result = parse_qsl(query_string)
  print(result)
  ```

  è¾“å‡ºï¼š

  ```python
  [
      ('q', 'python & java'),  # + å’Œ %26 è¢«è§£ç 
      ('page', '1'),
      ('lang', 'en'),
      ('lang', 'zh')          # é‡å¤é”®ä¿ç•™
  ]
  ```

###### å¯¹æ¯”æ€»ç»“

| å‡½æ•°             | è¾“å…¥       | è¾“å‡º                     | å¤„ç† `+`        | é€‚ç”¨åœºæ™¯                   |
| :--------------- | :--------- | :----------------------- | :-------------- | :------------------------- |
| `unquote()`      | ç¼–ç å­—ç¬¦ä¸² | å­—ç¬¦ä¸²                   | âŒ               | è·¯å¾„ã€Fragment             |
| `unquote_plus()` | ç¼–ç å­—ç¬¦ä¸² | å­—ç¬¦ä¸²                   | âœ…ï¼ˆ`+` â†’ ç©ºæ ¼ï¼‰ | æŸ¥è¯¢å‚æ•°                   |
| `parse_qs()`     | æŸ¥è¯¢å­—ç¬¦ä¸² | å­—å…¸ï¼ˆå€¼è‡ªåŠ¨åˆå¹¶ä¸ºåˆ—è¡¨ï¼‰ | âœ…               | è§£æå¤šå€¼å‚æ•°ï¼ˆå¦‚è¡¨å•æ•°æ®ï¼‰ |
| `parse_qsl()`    | æŸ¥è¯¢å­—ç¬¦ä¸² | å…ƒç»„åˆ—è¡¨ï¼ˆä¿ç•™é‡å¤é”®ï¼‰   | âœ…               | éœ€è¦ä¿æŒé¡ºåºæˆ–é‡å¤é”®       |

### å¤„ç†ç›¸å¯¹URL(urljoin)

`urljoin`å‡½æ•°ç”¨äºå°†ä¸€ä¸ªç›¸å¯¹ URL å’Œä¸€ä¸ªåŸºç¡€ URL åˆå¹¶æˆä¸€ä¸ªå®Œæ•´çš„ URLã€‚

```python
import urllib.parse

base_url = 'https://www.example.com/path/to/'
relative_url = 'resource'
full_url = urllib.parse.urljoin(base_url, relative_url)
print(full_url)
```

ä»£ç ä¸­ï¼Œ`urljoin`å‡½æ•°å°†ç›¸å¯¹ URL`relative_url`åˆå¹¶åˆ°åŸºç¡€ URL`base_url`ä¸Šï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ URLã€‚

## urllib.robotparser

`urllib.robotparser`æ˜¯ Python æ ‡å‡†åº“`urllib`ä¸­çš„ä¸€ä¸ªå­æ¨¡å—ï¼Œä¸»è¦ç”¨äºè§£æç½‘ç«™çš„`robots.txt`æ–‡ä»¶ã€‚`robots.txt`æ˜¯ä¸€ç§æ”¾ç½®åœ¨ç½‘ç«™æ ¹ç›®å½•ä¸‹çš„æ–‡æœ¬æ–‡ä»¶ï¼Œç½‘ç«™ç®¡ç†å‘˜å¯ä»¥é€šè¿‡è¯¥æ–‡ä»¶æ¥æŒ‡å®šæœç´¢å¼•æ“çˆ¬è™«æˆ–å…¶ä»–ç½‘ç»œæœºå™¨äººå¯ä»¥è®¿é—®å“ªäº›é¡µé¢ï¼Œä¸å¯ä»¥è®¿é—®å“ªäº›é¡µé¢ï¼Œä»¥æ­¤æ¥ä¿æŠ¤ç½‘ç«™çš„éšç§å’Œæ§åˆ¶çˆ¬è™«çš„è®¿é—®è¡Œä¸ºã€‚

#### 1. åŸºæœ¬ä½¿ç”¨æµç¨‹

- **åˆ›å»º`RobotFileParser`å¯¹è±¡**ï¼šä½¿ç”¨`urllib.robotparser.RobotFileParser()`åˆ›å»ºä¸€ä¸ªè§£æå™¨å¯¹è±¡ã€‚
- **è®¾ç½®`robots.txt`æ–‡ä»¶çš„ URL**ï¼šè°ƒç”¨`set_url()`æ–¹æ³•è®¾ç½®è¦è§£æçš„`robots.txt`æ–‡ä»¶çš„ URLã€‚
- **è¯»å–å¹¶è§£æ`robots.txt`æ–‡ä»¶**ï¼šè°ƒç”¨`read()`æ–¹æ³•è¯»å–å¹¶è§£æ`robots.txt`æ–‡ä»¶å†…å®¹ã€‚
- **æ£€æŸ¥æ˜¯å¦å…è®¸è®¿é—®**ï¼šä½¿ç”¨`can_fetch()`æ–¹æ³•æ£€æŸ¥æŒ‡å®šçš„çˆ¬è™«ï¼ˆç”±ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²æ ‡è¯†ï¼‰æ˜¯å¦è¢«å…è®¸è®¿é—®æŸä¸ª URLã€‚

#### 2. ç¤ºä¾‹ä»£ç 

```python
import urllib.robotparser

# åˆ›å»ºRobotFileParserå¯¹è±¡
rp = urllib.robotparser.RobotFileParser()

# è®¾ç½®robots.txtæ–‡ä»¶çš„URL
rp.set_url('https://www.example.com/robots.txt')

# è¯»å–å¹¶è§£ærobots.txtæ–‡ä»¶
rp.read()

# æ£€æŸ¥æ˜¯å¦å…è®¸è®¿é—®æŸä¸ªURL
user_agent = '*'  # è¡¨ç¤ºæ‰€æœ‰çˆ¬è™«
url = 'https://www.example.com/some-page'
can_fetch = rp.can_fetch(user_agent, url)

if can_fetch:
    print(f"å…è®¸è®¿é—® {url}")
else:
    print(f"ä¸å…è®¸è®¿é—® {url}")
```

#### 3. ä»£ç è§£é‡Š

- **`set_url(url)`**ï¼šè¯¥æ–¹æ³•ç”¨äºæŒ‡å®šè¦è§£æçš„`robots.txt`æ–‡ä»¶çš„ URLã€‚åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œé€šå¸¸æ˜¯ç½‘ç«™çš„æ ¹ç›®å½•ä¸‹çš„`robots.txt`æ–‡ä»¶ï¼Œä¾‹å¦‚`https://www.example.com/robots.txt`ã€‚
- **`read()`**ï¼šæ­¤æ–¹æ³•ä¼šä»æŒ‡å®šçš„ URL è¯»å–`robots.txt`æ–‡ä»¶çš„å†…å®¹ï¼Œå¹¶è¿›è¡Œè§£æã€‚è§£æåï¼Œ`RobotFileParser`å¯¹è±¡ä¼šå°†è§„åˆ™å­˜å‚¨åœ¨å†…éƒ¨ï¼Œä»¥ä¾¿åç»­çš„æ£€æŸ¥ã€‚
- **`can_fetch(useragent, url)`**ï¼šè¯¥æ–¹æ³•ç”¨äºæ£€æŸ¥æŒ‡å®šçš„ç”¨æˆ·ä»£ç†ï¼ˆå³çˆ¬è™«çš„æ ‡è¯†ï¼‰æ˜¯å¦è¢«å…è®¸è®¿é—®æŒ‡å®šçš„ URLã€‚`useragent`å‚æ•°å¯ä»¥æ˜¯å…·ä½“çš„çˆ¬è™«åç§°ï¼Œå¦‚`Googlebot`ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨`*`è¡¨ç¤ºæ‰€æœ‰çˆ¬è™«ã€‚å¦‚æœå…è®¸è®¿é—®ï¼Œè¿”å›`True`ï¼›å¦åˆ™è¿”å›`False`ã€‚

#### 4. æ³¨æ„äº‹é¡¹

- **ç½‘ç»œè¯·æ±‚é—®é¢˜**ï¼šå¦‚æœ`robots.txt`æ–‡ä»¶æ— æ³•è®¿é—®ï¼ˆä¾‹å¦‚ç½‘ç»œæ•…éšœã€æ–‡ä»¶ä¸å­˜åœ¨ç­‰ï¼‰ï¼Œ`read()`æ–¹æ³•å¯èƒ½ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œéœ€è¦è¿›è¡Œé€‚å½“çš„é”™è¯¯å¤„ç†ã€‚
- **è§„åˆ™æ›´æ–°**ï¼š`robots.txt`æ–‡ä»¶çš„è§„åˆ™å¯èƒ½ä¼šéšæ—¶æ›´æ–°ï¼Œå› æ­¤åœ¨çˆ¬è™«è¿è¡Œè¿‡ç¨‹ä¸­ï¼Œå¯èƒ½éœ€è¦å®šæœŸé‡æ–°è¯»å–å’Œè§£æè¯¥æ–‡ä»¶ã€‚

é€šè¿‡ä½¿ç”¨`urllib.robotparser`ï¼Œçˆ¬è™«å¼€å‘è€…å¯ä»¥ç¡®ä¿è‡ªå·±çš„çˆ¬è™«éµå®ˆç½‘ç«™çš„è®¿é—®è§„åˆ™ï¼Œé¿å…å› è¿åè§„åˆ™è€Œè¢«ç½‘ç«™å°ç¦ã€‚

## urllib.error

`urllib.error` æ¨¡å—ç”¨äº **å¤„ç† HTTP è¯·æ±‚è¿‡ç¨‹ä¸­å¯èƒ½å‘ç”Ÿçš„å¼‚å¸¸**ã€‚å®ƒæä¾›äº†ä¸¤ç§ä¸»è¦çš„å¼‚å¸¸ç±»ï¼Œå¸®åŠ©å¼€å‘è€…æ›´ä¼˜é›…åœ°æ•è·å’Œå¤„ç†ç½‘ç»œè¯·æ±‚é”™è¯¯ã€‚

### **1. æ ¸å¿ƒå¼‚å¸¸ç±»**

#### (1) URLErrorï¼ˆåŸºç¡€å¼‚å¸¸ï¼‰

- **è§¦å‘æ¡ä»¶**ï¼š
  - ç½‘ç»œé—®é¢˜ï¼ˆå¦‚ DNS è§£æå¤±è´¥ã€è¿æ¥è¶…æ—¶ã€æœåŠ¡å™¨æ— å“åº”ï¼‰ã€‚
  - æ— æ•ˆçš„ URL æ ¼å¼ã€‚
- **å±æ€§**ï¼š
  - `reason`ï¼šè¿”å›é”™è¯¯åŸå› ï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²æˆ–å­å¼‚å¸¸å¯¹è±¡ï¼Œå¦‚ `socket.error`ï¼‰ã€‚

+ **ç¤ºä¾‹**

```python
from urllib.request import urlopen
from urllib.error import URLError

try:
    response = urlopen("http://this-url-does-not-exist.com")
except URLError as e:
    print(f"è¯·æ±‚å¤±è´¥ï¼ŒåŸå› : {e.reason}")
    # è¾“å‡ºç¤ºä¾‹: "è¯·æ±‚å¤±è´¥ï¼ŒåŸå› : [Errno -2] Name or service not known"
```

#### (2) HTTPErrorï¼ˆ`URLError` çš„å­ç±»ï¼‰

- **è§¦å‘æ¡ä»¶**ï¼š
  - æœåŠ¡å™¨è¿”å›äº† **HTTP é”™è¯¯çŠ¶æ€ç **ï¼ˆå¦‚ 404ã€500 ç­‰ï¼‰ã€‚
- **å±æ€§**ï¼š
  - `code`ï¼šHTTP çŠ¶æ€ç ï¼ˆå¦‚ 404ï¼‰ã€‚
  - `reason`ï¼šé”™è¯¯æè¿°ï¼ˆå¦‚ "Not Found"ï¼‰ã€‚
  - `headers`ï¼šæœåŠ¡å™¨çš„å“åº”å¤´ï¼ˆå¯é€‰ï¼‰ã€‚

+ **ç¤ºä¾‹**

```python
from urllib.request import urlopen
from urllib.error import HTTPError

try:
    response = urlopen("https://httpbin.org/status/404")
except HTTPError as e:
    print(f"æœåŠ¡å™¨è¿”å›é”™è¯¯çŠ¶æ€ç : {e.code} {e.reason}")
    # è¾“å‡º: "æœåŠ¡å™¨è¿”å›é”™è¯¯çŠ¶æ€ç : 404 NOT FOUND"
```

### 2. å¼‚å¸¸å¤„ç†çš„æœ€ä½³å®è·µ

#### (1) ä¼˜å…ˆæ•è· `HTTPError`ï¼Œå†æ•è· `URLError`

ç”±äº `HTTPError` æ˜¯ `URLError` çš„å­ç±»ï¼Œé¡ºåºå¾ˆé‡è¦ï¼š

```python
try:
    response = urlopen("https://example.com/invalid-page")
except HTTPError as e:
    print(f"HTTP é”™è¯¯: {e.code} {e.reason}")
except URLError as e:
    print(f"URL é”™è¯¯: {e.reason}")
else:
    print("è¯·æ±‚æˆåŠŸï¼")
```

#### (2) æ£€æŸ¥ `reason` çš„å…·ä½“ç±»å‹

`URLError.reason` å¯èƒ½æ˜¯å…¶ä»–å¼‚å¸¸ï¼ˆå¦‚ `socket.timeout`ï¼‰ï¼š

```python
try:
    response = urlopen("https://example.com", timeout=0.001)  # æçŸ­è¶…æ—¶
except URLError as e:
    if isinstance(e.reason, socket.timeout):
        print("è¯·æ±‚è¶…æ—¶ï¼")
```

### 3. å¸¸è§é”™è¯¯åœºæ™¯

| å¼‚å¸¸ç±»å‹         | å…¸å‹åŸå›                | è§£å†³æ–¹æ¡ˆ                  |
| :--------------- | :--------------------- | :------------------------ |
| `HTTPError 404`  | é¡µé¢ä¸å­˜åœ¨             | æ£€æŸ¥ URL æˆ–é‡è¯•           |
| `HTTPError 500`  | æœåŠ¡å™¨å†…éƒ¨é”™è¯¯         | è”ç³»æœåŠ¡æä¾›å•†æˆ–é‡è¯•      |
| `URLError`       | ç½‘ç»œæ–­å¼€ã€DNS è§£æå¤±è´¥ | æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ– URL æ ¼å¼   |
| `socket.timeout` | è¯·æ±‚è¶…æ—¶               | å¢åŠ  `timeout` å‚æ•°æˆ–é‡è¯• |

## urllib.request

ç”¨äºæ‰“å¼€å’Œè¯»å– URLï¼Œæä¾›äº†é«˜çº§çš„æ¥å£æ¥å¤„ç†ä¸åŒç±»å‹çš„è¯·æ±‚ï¼Œåƒ HTTPã€FTP ç­‰åè®®çš„è¯·æ±‚ã€‚

### å‘é€ç®€å•çš„ GET è¯·æ±‚

ä½¿ç”¨ `urlopen` å‡½æ•°å¯ä»¥ç›´æ¥å‘é€ GET è¯·æ±‚å¹¶è·å–å“åº”å†…å®¹ã€‚

```python
import urllib.request

#(1)å®šä¹‰ä¸€ä¸ªurl å°±æ˜¯ä½ è¦è®¿é—®çš„åœ°å€
url = "https://www.baidu11.com"

#(2) æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
response = urllib.request.urlopen(url)

#(3) è·å–å“åº”ä¸­çš„é¡µé¢æºç 
content = response.read().decode("utf8")

#(4) æ‰“å°æ•°æ®
print(content)
```

åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼Œ`urlopen` å‡½æ•°æ‰“å¼€æŒ‡å®šçš„ URL å¹¶è¿”å›ä¸€ä¸ªå“åº”å¯¹è±¡ï¼Œé€šè¿‡è°ƒç”¨è¯¥å¯¹è±¡çš„ `read` æ–¹æ³•è¯»å–å“åº”å†…å®¹ï¼Œæœ€åä½¿ç”¨ `decode` æ–¹æ³•å°†å­—èŠ‚æ•°æ®è§£ç ä¸ºå­—ç¬¦ä¸²ã€‚è‹¥è¯·æ±‚è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œä¼šæ•è· `urllib.error.URLError` å¼‚å¸¸å¹¶è¾“å‡ºé”™è¯¯ä¿¡æ¯ã€‚

### å‘é€å¸¦è¯·æ±‚å¤´çš„ GET è¯·æ±‚

æœ‰æ—¶å€™éœ€è¦åœ¨è¯·æ±‚ä¸­æ·»åŠ è‡ªå®šä¹‰çš„è¯·æ±‚å¤´ï¼Œæ¯”å¦‚è®¾ç½® `User-Agent` æ¥æ¨¡æ‹Ÿä¸åŒçš„æµè§ˆå™¨ã€‚

> UAä»‹ç» ï¼šUser Agentä¸­æ–‡åä¸ºç”¨æˆ·ä»£ç† ï¼Œç®€ç§°  UAï¼Œå®ƒæ˜¯ä¸€ä¸ªç‰¹æ®Šå­—ç¬¦ä¸²å¤´ ï¼Œä½¿å¾—æœåŠ¡å™¨èƒ½å¤Ÿè¯†åˆ«å®¢æˆ·ä½¿ç”¨çš„æ“ä½œç³»ç»Ÿ åŠç‰ˆæœ¬ã€CPU ç±»å‹ã€æµè§ˆå™¨åŠç‰ˆæœ¬ã€‚æµè§ˆå™¨å†…æ ¸ã€æµè§ˆå™¨æ¸²æŸ“å¼•æ“ã€æµè§ˆå™¨è¯­è¨€ã€æµè§ˆå™¨æ’ä»¶ç­‰

```python
import urllib.request

url = 'https://www.example.com'
# åˆ›å»ºè¯·æ±‚å¯¹è±¡
req = urllib.request.Request(url)
# æ·»åŠ è¯·æ±‚å¤´
req.add_header('User-Agent', 'Mozilla/5.0')
try:
    response = urllib.request.urlopen(req)
    html = response.read().decode('utf-8')
    print(html)
except urllib.error.URLError as e:
    print(f"è¯·æ±‚å‡ºé”™: {e}")
```

æ­¤ä»£ç ä¸­ï¼Œå…ˆåˆ›å»ºäº†ä¸€ä¸ª `Request` å¯¹è±¡ï¼Œç„¶åä½¿ç”¨ `add_header` æ–¹æ³•æ·»åŠ äº†ä¸€ä¸ª `User - Agent` è¯·æ±‚å¤´ï¼Œæœ€åå°†è¯¥è¯·æ±‚å¯¹è±¡ä¼ é€’ç»™ `urlopen` å‡½æ•°å‘é€è¯·æ±‚ã€‚

ä¹Ÿå¯ä»¥é€šè¿‡å­—å…¸å…ˆæŠŠæ‰€æœ‰å¤´å‡†å¤‡å¥½ï¼Œç„¶åä¸€èµ·æ·»åŠ ï¼š

```python
#(2) æ·»åŠ è¯·æ±‚å¤´
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'
}
req = urllib.request.Request(url, headers=headers, method='GET')

#(2) å‘é€è¯·æ±‚
response = urllib.request.urlopen(req)
```

## çˆ¬è™«å°æ¡ˆä¾‹

### ç™¾åº¦æœç´¢

```python
import urllib.parse
import urllib.request

url = 'http://www.baidu.com/s?wd=' + urllib.parse.quote('å°åˆš')

headers={
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36 Edg/136.0.0.0'
}

req = urllib.request.Request(url, headers=headers)
#print(req.full_url)

resp =  urllib.request.urlopen(req)

content =  resp.read().decode('utf8')

with open('baidu.html','w',encoding='utf8') as file:
    file.write(content)
```

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œé¦–å…ˆå®šä¹‰äº†è¦æäº¤çš„æ•°æ®ï¼Œä½¿ç”¨ `urllib.parse.urlencode` å¯¹æ•°æ®è¿›è¡Œç¼–ç ï¼Œå†å°†å…¶è½¬æ¢ä¸ºå­—èŠ‚ç±»å‹ã€‚æ¥ç€åˆ›å»ºä¸€ä¸ª `Request` å¯¹è±¡ï¼ŒæŒ‡å®šè¯·æ±‚æ–¹æ³•ä¸º `POST`ï¼Œå¹¶å°†ç¼–ç åçš„æ•°æ®ä½œä¸ºè¯·æ±‚ä½“ï¼Œæœ€åå‘é€è¯·æ±‚å¹¶å¤„ç†å“åº”ã€‚

### ä¸‹è½½å›¾ç‰‡

æŠŠç™¾åº¦æœç´¢é¦–é¡µçš„å›¾ç‰‡çˆ¬ä¸‹æ¥ã€‚

```python
url = 'https://www.baidu.com/img/PCtm_d9c8750bed0b3c7d089fa7d55720d6cf.png'

headers={
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36 Edg/136.0.0.0'
}

req = urllib.request.Request(url, headers=headers)

resp =  urllib.request.urlopen(req)

content =  resp.read()

with open('baidu.png','wb') as file:
    file.write(content)
```

### ä¸‹è½½è§†é¢‘

åœ¨å¥½çœ‹è§†é¢‘éšä¾¿ä¸‹è½½ä¸€ä¸ªè§†é¢‘ã€‚

```python
url = 'https://vdept3.bdstatic.com/mda-nkuhe4s3ja12t07i/360p/h264/1669724352552074273/mda-nkuhe4s3ja12t07i.mp4?v_from_s=hkapp-haokan-nanjing&auth_key=1746526848-0-0-c3d95151aaa5a772a603c500cedc1af7&bcevod_channel=searchbox_feed&pd=1&cr=0&cd=0&pt=3&logid=1248522976&vid=9245979460455935030&klogid=1248522976&abtest='

headers={
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36 Edg/136.0.0.0'
}

req = urllib.request.Request(url, headers=headers)

resp =  urllib.request.urlopen(req)

content =  resp.read()

with open('baidu.mp4','wb') as file:
    file.write(content)
```

### ç™¾åº¦ç¿»è¯‘

#### ç®€å•ç¿»è¯‘

```python
url = 'https://fanyi.baidu.com/sug'

headers={
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36 Edg/136.0.0.0'
}

data={
    'kw':'ä½ å¥½'
}
#urlç¼–ç å¹¶è½¬æˆutf8çš„äºŒè¿›åˆ¶
data = urllib.parse.urlencode(data).encode()

req = urllib.request.Request(url, headers=headers,method='POST')

resp =  urllib.request.urlopen(req,data=data)

content =  resp.read().decode('utf8')

print(content)
```

#### è¯‘æ–‡è¯»éŸ³

```python
text = input('input english>')

url = f'https://fanyi.baidu.com/gettts?lan=uk&text={urllib.parse.quote(text)}&spd=3'

headers={
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36 Edg/136.0.0.0'
}

req = urllib.request.Request(url,headers=headers,method='GET')

resp =  urllib.request.urlopen(req)

content =  resp.read()

with open('translate.mp3','wb') as file:
    file.write(content)

print('ç¿»è¯‘å‘éŸ³ä¸‹è½½å®Œæˆ~')
```

#### ç™¾åº¦è¯¦ç»†ç¿»è¯‘

```python
import urllib.request import urllib.parse

url = 'https://fanyi.baidu.com/v2transapi ' headers = {
# ':authority ': 'fanyi.baidu.com ', # ':method ': 'POST ',
# ':path ': '/v2transapi ', # ':scheme ': 'https ',
# 'accept ': '*/* ',
# 'acceptâ€encoding ': 'gzip, deflate, br ', # 'acceptâ€language ': 'zhâ€CN,zh;q=0.9 ',
# 'contentâ€length ': '119 ',
# 'contentâ€type ': 'application/xâ€wwwâ€formâ€urlencoded; charset=UTFâ€8 ',


'cookie ': 'REALTIME_TRANS_SWITCH=1; FANYI_WORD_SWITCH=1; HISTORY_SWITCH=1;
SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; PSTM=1537097513;
BIDUPSID=D96F9A49A8630C54630DD60CE082A55C; BAIDUID=0814C35D13AE23F5EAFA8E0B24D9B436:FG=1;
to_lang_often=%5B%7B%22value%22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%2C%7B%22value%22 %3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%5D;
from_lang_often=%5B%7B%22value%22%3A%22zh%22%2C%22text%22%3A%22%u4E2D%u6587%22%7D%2C%7B%22value% 22%3A%22en%22%2C%22text%22%3A%22%u82F1%u8BED%22%7D%5D; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598;   delPer=0; H_PS_PSSID=1424_21115_29522_29519_29099_29568_28835_29220_26350; PSINO=2; locale=zh;
Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1563000604,1563334706,1565592510; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1565592510;
yj s_js_security_passport=2379b52646498f3b5d216e6b21c6f1c7bf00f062_1565592544_js ',
# 'origin ': 'https://fanyi.baidu.com ',
# 'referer ': 'https://fanyi.baidu.com/translate?
aldtype=16047&query=&keyfrom=baidu&smartresult=dict&lang=auto2zh ',
# 'secâ€fetchâ€mode ': 'cors ',
# 'secâ€fetchâ€site ': 'sameâ€origin ',
# 'userâ€agent ': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36 ',
# 'xâ€requestedâ€with ': 'XMLHttpRequest ',
}
data = {
		'from ': 'en ',
    	'to ': 'zh ',
		'query':'you',
		'transtype ': 'realtime ', 
    	'simple_means_flag ': '3 ', 
    	'sign ': '269482.65435 ',
		'token ': '2e0f1cb44414248f3a2b49fbad28bbd5 '
}
#å‚æ•°çš„ç¼–ç 
data = urllib.parse.urlencode(data).encode('utfâ€8 ')
# è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
request = urllib.request.Request(url=url,headers=headers,data=data) response = urllib.request.urlopen(request)
# è¯·æ±‚ä¹‹åè¿”å›çš„æ‰€æœ‰çš„æ•°æ®
content = response.read().decode('utfâ€8 ') import json
# loadså°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºpythonå¯¹è±¡
obj = json.loads(content)
# pythonå¯¹è±¡è½¬æ¢ä¸ºjsonå­—ç¬¦ä¸²    ensure_ascii=False  å¿½ç•¥å­—ç¬¦é›†ç¼–ç 
s = json.dumps(obj,ensure_ascii=False) print(s)    
```

### è±†ç“£ç”µå½±

çˆ¬å–è±†ç“£ç”µå½±æ’è¡Œç‰ˆå‰åé¡µæ•°æ®ã€‚

 ```python
 # çˆ¬å–è±†ç“£ç”µå½±å‰10é¡µæ•°æ®
 # https://movie.douban.com/j/chart/top_list?
 type=20&interval_id=100%3A90&action=&start=0&limit=20 # https://movie.douban.com/j/chart/top_list?
 type=20&interval_id=100%3A90&action=&start=20&limit=20 # https://movie.douban.com/j/chart/top_list?
 type=20&interval_id=100%3A90&action=&start=40&limit=20
 
 import urllib.request 
 import urllib.parse
 
 # ä¸‹è½½å‰10é¡µæ•°æ®
 # ä¸‹è½½çš„æ­¥éª¤ ï¼š1.è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶    2.è·å–å“åº”çš„æ•°æ®  3.ä¸‹è½½
 # æ¯æ‰§è¡Œä¸€æ¬¡è¿”å›ä¸€ä¸ªrequestå¯¹è±¡
 def create_request(page):
 	base_url = 'https://movie.douban.com/j/chart/top_list?type=20&interval_id=100%3A90&action=& ' 
 	headers = {
 	'Userâ€Agent ': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36 '
 	}
 	data={
 		'start ':(pageâ€1)*20,
 		'limit ':20 
 	}
 	# dataç¼–ç 
 	data = urllib.parse.urlencode(data) 
 	url = base_url + data
 	
 	request = urllib.request.Request(url=url,headers=headers) 
 	return request
 # è·å–ç½‘é¡µæºç 
 def get_content(request):
 	response = urllib.request.urlopen(request) 
     content = response.read().decode('utfâ€8 ')  
     return content
 
 def down_load(page,content):
 #    with openï¼ˆæ–‡ä»¶çš„åå­— ï¼Œæ¨¡å¼ ï¼Œç¼–ç ï¼‰as fp: #        fp.write(å†…å®¹)
 	with open('douban_ '+str(page)+ '.json ', 'w ',encoding= 'utfâ€8 ')as fp: 
         fp.write(content)
 
 if  __name__  == '__main__':
 	start_page = int(input('è¯·è¾“å…¥èµ·å§‹é¡µç ')) 
     end_page = int(input('è¯·è¾“å…¥ç»“æŸé¡µç '))
 	for page in range(start_page,end_page+1):
 		request = create_request(page) .
     	content = get_content(request) 
     	down_load(page,content)
 ```

### KFCå®˜ç½‘

```python
```

### CSDN

```python
eg:

import urllib.request import urllib.error
url = 'https://blog.csdn.net/ityard/article/details/102646738 '

# url = 'http://www.goudan11111.com '

headers = {
# 'Accept ':
'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,applicati on/signedâ€exchange;v=b3 ',
# 'Acceptâ€Encoding ': 'gzip, deflate, br ', # 'Acceptâ€Language ': 'zhâ€CN,zh;q=0.9 ',
# 'Cacheâ€Control ': 'maxâ€age=0 ',
# 'Connection ': 'keepâ€alive ',
'Cookie ': 'uuid_tt_dd=10_19284691370â€1530006813444â€566189;
smidV2=2018091619443662be2b30145de89bbb07f3f93a3167b80002b53e7acc61420;
_ga=GA1.2.1823123463.1543288103; dc_session_id=10_1550457613466.265727;
acw_tc=2760821d15710446036596250e10a1a7c89c3593e79928b22b3e3e2bc98b89;
Hm_lvt_e5ef47b9f471504959267fd614d579cd=1571329184;
Hm_ct_e5ef47b9f471504959267fd614d579cd=6525*1*10_19284691370â€1530006813444â€566189; 	 yadk_uid=r0LSXrcNYgymXooFiLaCGt1ahSCSxMCb;
Hm_lvt_6bcd52f51e9b3dce32bec4a3997715ac=1571329199,1571329223,1571713144,1571799968;
acw_sc    v2=5dafc3b3bc5fad549cbdea513e330fbbbee00e25; firstDie=1; SESSION=396bc85câ€556bâ€42bd â€ 890câ€c20adaaa1e47; UserName=weixin_42565646; UserInfo=d34ab5352bfa4f21b1eb68cdacd74768;
UserToken=d34ab5352bfa4f21b1eb68cdacd74768; UserNick=weixin_42565646; AU=7A5;
UN=weixin_42565646; BT=1571800370777; p_uid=U000000; dc_tos=pzt4xf; Hm_lpvt_6bcd52f51e9b3dce32bec4a3997715ac=1571800372;
Hm_ct_6bcd52f51e9b3dce32bec4a3997715ac=1788*1*PC_VC!6525*1*10_19284691370â€1530006813444 â€ 566189!5744*1*weixin_42565646;
announcement=%257B%2522isLogin%2522%253Atrue%252C%2522announcementUrl%2522%253A%2522https%253A%2 52F%252Fblogdev.blog.csdn.net%252Farticle%252Fdetails%252F102605809%2522%252C%2522announcementCo unt%2522%253A0%252C%2522announcementExpire%2522%253A3600000%257D ',
# 'Host ': 'blog.csdn.net ',
# 'Referer ': 'https://passport.csdn.net/login?code=public ',
# 'Secâ€Fetchâ€Mode ': 'navigate ',  # 'Secâ€Fetchâ€Site ': 'sameâ€site ', # 'Secâ€Fetchâ€User ': '?1 ',
# 'Upgradeâ€Insecureâ€Requests ': '1 ',
'Userâ€Agent ': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36 ',
} try:
request = urllib.request.Request(url=url,headers=headers)

response = urllib.request.urlopen(request)

content = response.read().decode('utfâ€8 ') print(content)
except urllib.error.HTTPError: print(1111)

except urllib.error.URLError: print(2222)
```

## 12.cookieç™»å½•

 cookieçš„ä¸­æ–‡ç¿»è¯‘æ˜¯æ›²å¥‡ï¼Œå°ç”œé¥¼çš„æ„æ€ã€‚cookieå…¶å®å°±æ˜¯ä¸€äº›æ•°æ®ä¿¡æ¯ï¼Œç±»å‹ä¸ºâ€œ**å°å‹æ–‡æœ¬æ–‡ä»¶**â€ï¼Œå­˜å‚¨äºç”µè„‘ä¸Šçš„æ–‡æœ¬æ–‡ä»¶ä¸­ã€‚

### **Cookieæœ‰ä»€ä¹ˆç”¨ï¼Ÿ**

æˆ‘ä»¬æƒ³è±¡ä¸€ä¸ªåœºæ™¯ï¼Œå½“æˆ‘ä»¬æ‰“å¼€ä¸€ä¸ªç½‘ç«™æ—¶ï¼Œå¦‚æœè¿™ä¸ªç½‘ç«™æˆ‘ä»¬æ›¾ç»ç™»å½•è¿‡ï¼Œé‚£ä¹ˆå½“æˆ‘ä»¬å†æ¬¡æ‰“å¼€ç½‘ç«™æ—¶ï¼Œå‘ç°å°±ä¸éœ€è¦å†æ¬¡ç™»å½•äº†ï¼Œè€Œæ˜¯ç›´æ¥è¿›å…¥äº†é¦–é¡µã€‚ä¾‹å¦‚bilibiliï¼Œcsdnç­‰ç½‘ç«™ã€‚

â€‹    è¿™æ˜¯æ€ä¹ˆåšåˆ°çš„å‘¢ï¼Ÿå…¶å®å°±æ˜¯æ¸¸è§ˆå™¨ä¿å­˜äº†æˆ‘ä»¬çš„cookieï¼Œé‡Œé¢è®°å½•äº†ä¸€äº›ä¿¡æ¯ï¼Œå½“ç„¶ï¼Œè¿™äº›cookieæ˜¯æœåŠ¡å™¨åˆ›å»ºåè¿”å›ç»™æ¸¸è§ˆå™¨çš„ã€‚æµè§ˆå™¨åªè¿›è¡Œäº†ä¿å­˜ã€‚

### **Cookieçš„è¡¨ç¤º** 

â€‹    ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œcookieæ˜¯ä»¥é”®å€¼å¯¹è¿›è¡Œè¡¨ç¤ºçš„(key-value)ï¼Œä¾‹å¦‚name=jackï¼Œè¿™ä¸ªå°±è¡¨ç¤ºcookieçš„åå­—æ˜¯nameï¼Œcookieæºå¸¦çš„å€¼æ˜¯jackã€‚

å½“éœ€è¦æŠ“å–ä¸€äº›ï¼Œç™»å½•ä¹‹åæ‰èƒ½è®¿é—®çš„ç½‘ç«™æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å…ˆç™»å½•ï¼Œè·å–cookieï¼Œç„¶ååœ¨çˆ¬å–æ—¶æºå¸¦cookieï¼ç„¶åå°±ä¸éœ€è¦è¾“å…¥è´¦å·å’Œå¯†ç äº†ï¼

### ä½¿ç”¨æ¡ˆä¾‹

#### 1.weiboç™»é™†

#### 2.qqç©ºé—´çš„çˆ¬å–

## 13.Handlerå¤„ç†å™¨

ä¸ºä»€ä¹ˆè¦å­¦ä¹ handlerï¼Ÿ

urllib.request.urlopen(url) ä¸èƒ½å®šåˆ¶è¯·æ±‚å¤´

urllib.request.Request(url,headers,data) å¯ä»¥å®šåˆ¶è¯·æ±‚å¤´

Handler

å®šåˆ¶æ›´é«˜çº§çš„è¯·æ±‚å¤´ï¼ˆéšç€ä¸šåŠ¡é€»è¾‘çš„å¤æ‚ è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶å·²ç»æ»¡è¶³ä¸äº†æˆ‘ä»¬çš„éœ€æ±‚ï¼ˆåŠ¨æ€cookieå’Œä»£ç† ä¸èƒ½ä½¿ç”¨è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶ï¼‰

```python
eg:
import urllib.request
url = 'http://www.baidu.com ' headers = {
'User â€ Agent ': 'Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, likeGecko) Chrome / 74.0.3729.169Safari / 537.36 '
}
request = urllib.request.Request(url=url,headers=headers) handler = urllib.request.HTTPHandler()
opener = urllib.request.build_opener(handler) response = opener.open(request)
print(response.read().decode('utfâ€8 '))
```

## 14.ä»£ç†æœåŠ¡å™¨

1.ä»£ç†çš„å¸¸ç”¨åŠŸèƒ½?

1.çªç ´è‡ªèº«IPè®¿é—®é™åˆ¶ ï¼Œè®¿é—®å›½å¤–ç«™ç‚¹ã€‚ 2.è®¿é—®ä¸€äº›å•ä½æˆ–å›¢ä½“å†…éƒ¨èµ„æº

æ‰©å±• ï¼šæŸå¤§å­¦FTP(å‰ææ˜¯è¯¥ä»£ç†åœ°å€åœ¨è¯¥èµ„æºçš„å…è®¸è®¿é—®èŒƒå›´ä¹‹å†…)ï¼Œä½¿ç”¨æ•™è‚²ç½‘å†…åœ°å€æ®µå…è´¹ä»£ç†æœåŠ¡ å™¨ ï¼Œå°±å¯ä»¥ç”¨äºå¯¹æ•™è‚²ç½‘å¼€æ”¾çš„å„ç±»FTPä¸‹è½½ä¸Šä¼  ï¼Œä»¥åŠå„ç±»èµ„æ–™æŸ¥è¯¢å…±äº«ç­‰æœåŠ¡ã€‚

3.æé«˜è®¿é—®é€Ÿåº¦

æ‰©å±• ï¼šé€šå¸¸ä»£ç†æœåŠ¡å™¨éƒ½è®¾ç½®ä¸€ä¸ªè¾ƒå¤§çš„ç¡¬ç›˜ç¼“å†²åŒº ï¼Œå½“æœ‰å¤–ç•Œçš„ä¿¡æ¯é€šè¿‡æ—¶ ï¼ŒåŒæ—¶ä¹Ÿå°†å…¶ä¿å­˜åˆ°ç¼“å†² åŒºä¸­ ï¼Œå½“å…¶ä»–ç”¨æˆ·å†è®¿é—®ç›¸åŒçš„ä¿¡æ¯æ—¶ ï¼Œ åˆ™ç›´æ¥ç”±ç¼“å†²åŒºä¸­å–å‡ºä¿¡æ¯ ï¼Œä¼ ç»™ç”¨æˆ· ï¼Œä»¥æé«˜è®¿é—®é€Ÿåº¦ã€‚

4.éšè—çœŸå®IP

æ‰©å±• ï¼šä¸Šç½‘è€…ä¹Ÿå¯ä»¥é€šè¿‡è¿™ç§æ–¹æ³•éšè—è‡ªå·±çš„IPï¼Œå…å—æ”»å‡»ã€‚ 2.ä»£ç é…ç½®ä»£ç†

åˆ›å»ºReuqestå¯¹è±¡

åˆ›å»ºProxyHandlerå¯¹è±¡

ç”¨handlerå¯¹è±¡åˆ›å»ºopenerå¯¹è±¡ ä½¿ç”¨opener.openå‡½æ•°å‘é€è¯·æ±‚

 ```python
 eg:
 import urllib.request
 url = 'http://www.baidu.com/s?wd=ip ' headers = {
 'User â€ Agent ': 'Mozilla / 5.0(Windows NT 10.0;Win64;x64) AppleWebKit / 537.36(KHTML, likeGecko) Chrome / 74.0.3729.169Safari / 537.36 '
 }
 request = urllib.request.Request(url=url,headers=headers) proxies = {'http ': '117.141.155.244:53281 '}
 handler = urllib.request.ProxyHandler(proxies=proxies)
 opener = urllib.request.build_opener(handler) response = opener.open(request)
 content = response.read().decode('utfâ€8 ')
 with open( 'daili.html ', 'w ',encoding= 'utfâ€8 ')as fp: fp.write(content)
 ```

æ‰©å±•ï¼š 1.ä»£ç†æ±  2.å¿«ä»£ç†